{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Classification Dataset\n",
    "\n",
    "http://cogcomp.cs.illinois.edu/Data/QA/QC/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting load_qc.py\n"
     ]
    }
   ],
   "source": [
    "%%file load_qc.py\n",
    "## Question Classification Dataset\n",
    "## http://cogcomp.cs.illinois.edu/Data/QA/QC/\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def split_question(question):\n",
    "    q = question.strip().split(\" \")\n",
    "    return (q[0],q[1:])\n",
    "file\n",
    "\n",
    "def load_question_file(filename):\n",
    "    f = open(filename)\n",
    "    X = list()\n",
    "    Y = list()\n",
    "    for line in f:\n",
    "        (y,x) = split_question(line)\n",
    "        Y.append(y)\n",
    "        X.append(x)\n",
    "    return (Y,X)\n",
    "\n",
    "def build_dict(sentences):\n",
    "#    from collections import OrderedDict\n",
    "\n",
    "    '''\n",
    "    Build dictionary of train words\n",
    "    Outputs: \n",
    "     - Dictionary of word --> word index\n",
    "     - Dictionary of word --> word count freq\n",
    "    '''\n",
    "    print 'Building dictionary..',\n",
    "    wordcount = dict()\n",
    "    #For each worn in each sentence, cummulate frequency\n",
    "    for ss in sentences:\n",
    "        for w in ss:\n",
    "            if w not in wordcount:\n",
    "                wordcount[w] = 1\n",
    "            else:\n",
    "                wordcount[w] += 1\n",
    "\n",
    "    counts = wordcount.values() # List of frequencies\n",
    "    keys = wordcount.keys() #List of words\n",
    "    \n",
    "    sorted_idx = reversed(np.argsort(counts))\n",
    "    \n",
    "    worddict = dict()\n",
    "    for idx, ss in enumerate(sorted_idx):\n",
    "        worddict[keys[ss]] = idx+2  # leave 0 and 1 (UNK)\n",
    "    print np.sum(counts), ' total words ', len(keys), ' unique words'\n",
    "\n",
    "    return worddict, wordcount\n",
    "\n",
    "def generate_sequence(sentences, dictionary):\n",
    "    '''\n",
    "    Convert tokenized text in sequences of integers\n",
    "    '''\n",
    "    seqs = [None] * len(sentences)\n",
    "    for idx, ss in enumerate(sentences):\n",
    "        seqs[idx] = [dictionary[w] if w in dictionary else 1 for w in ss]\n",
    "\n",
    "    return seqs\n",
    "\n",
    "def parse_label(label):\n",
    "    t = label.split(\":\")\n",
    "    return (t[0],t[1])\n",
    "\n",
    "def load_corpus(path): \n",
    "    (Y_train_full,X_train_sentences) = load_question_file(path + \"train_5500.label\")\n",
    "    (Y_test_full,X_test_sentences) = load_question_file(path + \"TREC_10.label\")\n",
    "    return (Y_train_full,X_train_sentences), (Y_test_full,X_test_sentences)\n",
    "\n",
    "def load_data(path): \n",
    "    (Y_train_full,X_train_sentences), (Y_test_full,X_test_sentences) = load_corpus(path)\n",
    "    worddict, wordcount = build_dict(X_train_sentences)\n",
    "    \n",
    "    X_train = generate_sequence(X_train_sentences, worddict)\n",
    "    X_test  = generate_sequence(X_test_sentences, worddict)\n",
    "    \n",
    "    Y_train_label = [parse_label(y)[0]  for y in Y_train_full]\n",
    "    Y_test_label  = [parse_label(y)[0]  for y in Y_test_full]\n",
    "    \n",
    "    labels = set(Y_train_label + Y_test_label)\n",
    "    catdict = {label: idx for (idx, label) in enumerate(labels)}\n",
    "    \n",
    "    Y_train = [catdict[y] for y in Y_train_label]\n",
    "    Y_test  = [catdict[y] for y in Y_test_label]\n",
    "    \n",
    "    return (Y_train,X_train), (Y_test,X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary.. 55635  total words  9448  unique words\n"
     ]
    }
   ],
   "source": [
    "path = \"/home/ec2-user/data/qc/\"\n",
    "(Y_train,X_train), (Y_test,X_test) = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num. classes:', 6)\n",
      "(5452, 'train sequences')\n",
      "(500, 'test sequences')\n",
      "('Max length train', 37)\n",
      "('Max length tet', 17)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = len(set(Y_train + Y_test))\n",
    "print('Num. classes:', nb_classes)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "maxlen_train = max([len(x) for x in X_train])\n",
    "maxlen_test  = max([len(x) for x in X_test])\n",
    "print('Max length train', maxlen_train)\n",
    "print('Max length tet', maxlen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load models.py\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "\n",
    "def build_lstm(max_features, embedding_dims, nb_classes):  \n",
    "   model = Sequential()\n",
    "\n",
    "   model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "   model.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
    "   model.add(Dense(nb_classes))\n",
    "   model.add(Activation('softmax'))\n",
    "\n",
    "   return model\n",
    "\n",
    "\n",
    "def build_cnn(embedding_dims, maxlen, nb_filter, filter_length, hidden_dims, nb_classes ) : \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen,\n",
    "                    dropout=0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn nb_filter\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    # we use max pooling:\n",
    "    model.add(MaxPooling1D(pool_length=model.output_shape[1]))\n",
    "\n",
    "    # We flatten the output of the conv layer,\n",
    "    # so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_cnn_lstm(embedding_size, maxlen, nb_filter, filter_length, pool_length, lstm_output_size, nb_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "     \n",
    "    return model     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "maxlen = 30  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_4 (Embedding)          (None, None, 128)     1280000     embedding_input_4[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                    (None, 128)           131584      embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 6)             774         lstm_4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 6)             0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,412,358\n",
      "Trainable params: 1,412,358\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_lstm(max_features, 128, nb_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 5452 samples, validate on 500 samples\n",
      "Epoch 1/15\n",
      "5452/5452 [==============================] - 9s - loss: 1.5024 - acc: 0.3612 - val_loss: 0.9760 - val_acc: 0.7500\n",
      "Epoch 2/15\n",
      "5452/5452 [==============================] - 8s - loss: 0.7676 - acc: 0.7351 - val_loss: 0.5576 - val_acc: 0.8420\n",
      "Epoch 3/15\n",
      "5452/5452 [==============================] - 9s - loss: 0.4169 - acc: 0.8701 - val_loss: 0.4504 - val_acc: 0.8620\n",
      "Epoch 4/15\n",
      "5452/5452 [==============================] - 8s - loss: 0.2939 - acc: 0.9090 - val_loss: 0.4013 - val_acc: 0.8760\n",
      "Epoch 5/15\n",
      "5452/5452 [==============================] - 8s - loss: 0.2315 - acc: 0.9312 - val_loss: 0.4440 - val_acc: 0.8600\n",
      "Epoch 6/15\n",
      "5452/5452 [==============================] - 9s - loss: 0.1812 - acc: 0.9455 - val_loss: 0.4253 - val_acc: 0.8800\n",
      "Epoch 7/15\n",
      "5452/5452 [==============================] - 8s - loss: 0.1540 - acc: 0.9521 - val_loss: 0.4114 - val_acc: 0.8900\n",
      "Epoch 8/15\n",
      "5452/5452 [==============================] - 9s - loss: 0.1429 - acc: 0.9571 - val_loss: 0.4888 - val_acc: 0.8600\n",
      "Epoch 9/15\n",
      "5452/5452 [==============================] - 9s - loss: 0.1248 - acc: 0.9639 - val_loss: 0.4882 - val_acc: 0.8780\n",
      "Epoch 10/15\n",
      "5452/5452 [==============================] - 8s - loss: 0.1206 - acc: 0.9655 - val_loss: 0.4797 - val_acc: 0.8600\n",
      "Epoch 11/15\n",
      "5452/5452 [==============================] - 9s - loss: 0.1158 - acc: 0.9633 - val_loss: 0.4866 - val_acc: 0.8680\n",
      "Epoch 12/15\n",
      "5452/5452 [==============================] - 9s - loss: 0.1005 - acc: 0.9675 - val_loss: 0.5028 - val_acc: 0.8720\n",
      "Epoch 13/15\n",
      "5452/5452 [==============================] - 9s - loss: 0.1050 - acc: 0.9652 - val_loss: 0.5061 - val_acc: 0.8660\n",
      "Epoch 14/15\n",
      "5452/5452 [==============================] - 8s - loss: 0.0870 - acc: 0.9727 - val_loss: 0.4859 - val_acc: 0.8660\n",
      "Epoch 15/15\n",
      "5452/5452 [==============================] - 8s - loss: 0.0885 - acc: 0.9732 - val_loss: 0.5583 - val_acc: 0.8520\n",
      "500/500 [==============================] - 0s     \n",
      "('Test score:', 0.55825018548965455)\n",
      "('Test accuracy:', 0.85199999999999998)\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 10000 # vocabulary size\n",
    "maxlen =  30         # max document length \n",
    "batch_size = 32      # minibatch size \n",
    "nb_epoch = 2         # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "embedding_dims = 128 # size of embedding dims\n",
    "nb_filter = 65      # number of filters \n",
    "filter_length = 5    # 1d convolution size\n",
    "hidden_dims = 250    # size of hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model = build_cnn(embedding_dims, maxlen, nb_filter, filter_length, hidden_dims, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, 30, 128)       1280000     embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_2 (Convolution1D)  (None, 26, 65)        41665       embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_2 (MaxPooling1D)    (None, 1, 65)         0           convolution1d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 65)            0           maxpooling1d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 250)           16500       flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 250)           0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 250)           0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 6)             1506        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 6)             0           dense_7[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,339,671\n",
      "Trainable params: 1,339,671\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 5452 samples, validate on 500 samples\n",
      "Epoch 1/15\n",
      "5452/5452 [==============================] - 1s - loss: 1.2290 - acc: 0.5193 - val_loss: 0.6122 - val_acc: 0.8460\n",
      "Epoch 2/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.5829 - acc: 0.7997 - val_loss: 0.3987 - val_acc: 0.8720\n",
      "Epoch 3/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.3263 - acc: 0.8942 - val_loss: 0.3326 - val_acc: 0.8920\n",
      "Epoch 4/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.1986 - acc: 0.9336 - val_loss: 0.3691 - val_acc: 0.8820\n",
      "Epoch 5/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.1622 - acc: 0.9497 - val_loss: 0.3394 - val_acc: 0.8800\n",
      "Epoch 6/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.1188 - acc: 0.9591 - val_loss: 0.4096 - val_acc: 0.8660\n",
      "Epoch 7/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.1091 - acc: 0.9648 - val_loss: 0.3766 - val_acc: 0.8820\n",
      "Epoch 8/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0986 - acc: 0.9675 - val_loss: 0.4262 - val_acc: 0.8700\n",
      "Epoch 9/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0935 - acc: 0.9699 - val_loss: 0.4411 - val_acc: 0.8640\n",
      "Epoch 10/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0743 - acc: 0.9754 - val_loss: 0.5140 - val_acc: 0.8700\n",
      "Epoch 11/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0794 - acc: 0.9740 - val_loss: 0.5533 - val_acc: 0.8600\n",
      "Epoch 12/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0584 - acc: 0.9802 - val_loss: 0.5990 - val_acc: 0.8440\n",
      "Epoch 13/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0747 - acc: 0.9773 - val_loss: 0.5331 - val_acc: 0.8580\n",
      "Epoch 14/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0545 - acc: 0.9833 - val_loss: 0.5064 - val_acc: 0.8580\n",
      "Epoch 15/15\n",
      "5452/5452 [==============================] - 1s - loss: 0.0553 - acc: 0.9824 - val_loss: 0.4618 - val_acc: 0.8720\n",
      "416/500 [=======================>......] - ETA: 0s('Test score:', 0.46179229307174685)\n",
      "('Test accuracy:', 0.87199999952316287)\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "cnn_model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = cnn_model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pool_length = 4\n",
    "lstm_output_size = 70\n",
    "\n",
    "cnn_lstm_model = build_cnn_lstm(embedding_dims, maxlen, nb_filter, filter_length, pool_length, lstm_output_size, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 30, 128)       1280000     embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 30, 128)       0           embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_4 (Convolution1D)  (None, 26, 65)        41665       dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_4 (MaxPooling1D)    (None, 6, 65)         0           convolution1d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                    (None, 70)            38080       maxpooling1d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 6)             426         lstm_6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 6)             0           dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,360,171\n",
      "Trainable params: 1,360,171\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_lstm_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 5452 samples, validate on 500 samples\n",
      "Epoch 1/15\n",
      "5452/5452 [==============================] - 3s - loss: 1.2851 - acc: 0.4862 - val_loss: 0.6875 - val_acc: 0.7440\n",
      "Epoch 2/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.4640 - acc: 0.8522 - val_loss: 0.4794 - val_acc: 0.8260\n",
      "Epoch 3/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.2038 - acc: 0.9411 - val_loss: 0.4902 - val_acc: 0.8400\n",
      "Epoch 4/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.1044 - acc: 0.9707 - val_loss: 0.4882 - val_acc: 0.8440\n",
      "Epoch 5/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0527 - acc: 0.9855 - val_loss: 0.5402 - val_acc: 0.8420\n",
      "Epoch 6/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0321 - acc: 0.9921 - val_loss: 0.5080 - val_acc: 0.8680\n",
      "Epoch 7/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0234 - acc: 0.9939 - val_loss: 0.5781 - val_acc: 0.8480\n",
      "Epoch 8/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0184 - acc: 0.9960 - val_loss: 0.5322 - val_acc: 0.8620\n",
      "Epoch 9/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0155 - acc: 0.9950 - val_loss: 0.5777 - val_acc: 0.8580\n",
      "Epoch 10/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0138 - acc: 0.9961 - val_loss: 0.6080 - val_acc: 0.8660\n",
      "Epoch 11/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0131 - acc: 0.9958 - val_loss: 0.6473 - val_acc: 0.8600\n",
      "Epoch 12/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0130 - acc: 0.9963 - val_loss: 0.6537 - val_acc: 0.8400\n",
      "Epoch 13/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0131 - acc: 0.9967 - val_loss: 0.6366 - val_acc: 0.8660\n",
      "Epoch 14/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0116 - acc: 0.9967 - val_loss: 0.6556 - val_acc: 0.8480\n",
      "Epoch 15/15\n",
      "5452/5452 [==============================] - 3s - loss: 0.0112 - acc: 0.9967 - val_loss: 0.6721 - val_acc: 0.8560\n",
      "480/500 [===========================>..] - ETA: 0s('Test score:', 0.67210372447967526)\n",
      "('Test accuracy:', 0.85599999999999998)\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "cnn_lstm_model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=15,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = cnn_lstm_model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
