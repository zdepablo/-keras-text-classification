{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: GRID K520 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "from load_sentiment import load_sentiment_data\n",
    "from text_utils import build_dict, generate_sequence\n",
    "from models import build_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dictionary.. 201209  total words  20247  unique words\n",
      "Filtering to 10000  unique words\n"
     ]
    }
   ],
   "source": [
    "dirpath = \"/home/ec2-user/data/rt-polaritydata/\"\n",
    "max_features = 10000\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, worddict = load_sentiment_data(dirpath, max_tokens=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Num. classes:', 2)\n",
      "(9595, 'train sequences')\n",
      "(1067, 'test sequences')\n",
      "('Max length train', 56)\n",
      "('Max length tet', 59)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = len(set(Y_train + Y_test))\n",
    "print('Num. classes:', nb_classes)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "maxlen_train = max([len(x) for x in X_train])\n",
    "maxlen_test  = max([len(x) for x in X_test])\n",
    "print('Max length train', maxlen_train)\n",
    "print('Max length tet', maxlen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 60  \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "('X_train shape:', (9595, 60))\n",
      "('X_test shape:', (1067, 60))\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "('Y_train shape:', (9595, 2))\n",
      "('Y_test shape:', (1067, 2))\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('Y_test shape:', Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, None, 64)      640000      embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 64)            33024       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 2)             130         lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 2)             0           dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 673,154\n",
      "Trainable params: 673,154\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_lstm(max_features, 64, nb_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/5\n",
      "9595/9595 [==============================] - 23s - loss: 0.6530 - acc: 0.6077 - val_loss: 0.5295 - val_acc: 0.7423\n",
      "Epoch 2/5\n",
      "9595/9595 [==============================] - 23s - loss: 0.4658 - acc: 0.7804 - val_loss: 0.4858 - val_acc: 0.7723\n",
      "Epoch 3/5\n",
      "9595/9595 [==============================] - 24s - loss: 0.3457 - acc: 0.8516 - val_loss: 0.5425 - val_acc: 0.7573\n",
      "Epoch 4/5\n",
      "9595/9595 [==============================] - 23s - loss: 0.2772 - acc: 0.8856 - val_loss: 0.5756 - val_acc: 0.7563\n",
      "Epoch 5/5\n",
      "9595/9595 [==============================] - 23s - loss: 0.2316 - acc: 0.9035 - val_loss: 0.5631 - val_acc: 0.7498\n",
      "1067/1067 [==============================] - 1s     \n",
      "('Test score:', 0.56307400929223961)\n",
      "('Test accuracy:', 0.74976569816344452)\n"
     ]
    }
   ],
   "source": [
    "print('Train...')\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=5,\n",
    "          validation_data=(X_test, Y_test))\n",
    "score, acc = model.evaluate(X_test, Y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from models import build_cnn, build_cnn_lstm, build_bidirectional_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load models.py\n",
    "\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import Input, Bidirectional\n",
    "\n",
    "def build_lstm(max_features, embedding_dims, nb_classes):  \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dims, dropout=0.2))\n",
    "    model.add(LSTM(embedding_dims, dropout_W=0.2, dropout_U=0.2)) \n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_cnn(max_features, embedding_dims, maxlen, nb_filter, filter_length, hidden_dims, nb_classes ) : \n",
    "    model = Sequential()\n",
    "    # we start off with an efficient embedding layer which maps\n",
    "    # our vocab indices into embedding_dims dimensions\n",
    "    model.add(Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen,\n",
    "                    dropout=0.2))\n",
    "\n",
    "    # we add a Convolution1D, which will learn nb_filter\n",
    "    # word group filters of size filter_length:\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    # we use max pooling:\n",
    "    model.add(MaxPooling1D(pool_length=model.output_shape[1]))\n",
    "\n",
    "    # We flatten the output of the conv layer,\n",
    "    # so that we can add a vanilla dense layer:\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    model.add(Dense(hidden_dims))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    # We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_cnn_lstm(embedding_size, maxlen, nb_filter, filter_length, pool_length, lstm_output_size, nb_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution1D(nb_filter=nb_filter,\n",
    "                        filter_length=filter_length,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1))\n",
    "    model.add(MaxPooling1D(pool_length=pool_length))\n",
    "    model.add(LSTM(lstm_output_size))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "     \n",
    "    return model     \n",
    "\n",
    "def build_bidirectional_lstm(embedding_dims, lstm_output_size, nb_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
    "    model.add(Bidirectional(LSTM(lstm_output_size)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "# set parameters:\n",
    "embedding_dims = 128 # size of embedding dims\n",
    "nb_filter = 65      # number of filters \n",
    "filter_length = 5    # 1d convolution size\n",
    "hidden_dims = 250    # size of hidden layers \n",
    "\n",
    "\n",
    "print('Build model...')\n",
    "cnn_model = build_cnn(max_features,embedding_dims, maxlen, nb_filter, filter_length, hidden_dims, nb_classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 60, 128)       1280000     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_1 (Convolution1D)  (None, 56, 65)        41665       embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_1 (MaxPooling1D)    (None, 1, 65)         0           convolution1d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 65)            0           maxpooling1d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 250)           16500       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 250)           0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 250)           0           dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 2)             502         activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 2)             0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 1,338,667\n",
      "Trainable params: 1,338,667\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn_model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9595 samples, validate on 1067 samples\n",
      "Epoch 1/5\n",
      "9595/9595 [==============================] - 3s - loss: 0.0956 - acc: 0.9637 - val_loss: 0.7908 - val_acc: 0.7563\n",
      "Epoch 2/5\n",
      "9595/9595 [==============================] - 3s - loss: 0.0937 - acc: 0.9637 - val_loss: 0.8952 - val_acc: 0.7479\n",
      "Epoch 3/5\n",
      "9595/9595 [==============================] - 3s - loss: 0.0804 - acc: 0.9688 - val_loss: 0.7793 - val_acc: 0.7498\n",
      "Epoch 4/5\n",
      "9595/9595 [==============================] - 3s - loss: 0.0682 - acc: 0.9745 - val_loss: 0.8773 - val_acc: 0.7310\n",
      "Epoch 5/5\n",
      "9595/9595 [==============================] - 3s - loss: 0.0657 - acc: 0.9757 - val_loss: 0.8017 - val_acc: 0.7385\n"
     ]
    }
   ],
   "source": [
    "nb_epoch = 5\n",
    "\n",
    "history = cnn_model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
